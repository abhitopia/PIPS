{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pips.grid_dataset import GridDataset, DatasetType\n",
    "\n",
    "pad_token_id = 10\n",
    "batch_size = 32\n",
    "H, W = 32, 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GridDataset(dataset_type=DatasetType.TRAIN)\n",
    "val_dataset = GridDataset(dataset_type=DatasetType.VAL)\n",
    "\n",
    "def collate_fn(batch, permute=False):\n",
    "    result = [] \n",
    "\n",
    "    for g in batch:\n",
    "        if permute: \n",
    "            g = g.permute()\n",
    "        result.append(g.project(H, W, pad_token_id).flatten())\n",
    "\n",
    "    batch = np.stack(result)\n",
    "\n",
    "    return torch.from_numpy(batch).to(device)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          collate_fn=lambda x: collate_fn(x, permute=True))\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size, \n",
    "                        collate_fn=lambda x: collate_fn(x, permute=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pips.dvae import AttnCodebook, LatentTransformer, RoPE2D, RotaryPositionalEmbeddings, Transformer\n",
    "\n",
    "\n",
    "class GridDVAEConfig:\n",
    "    def __init__(self):\n",
    "        self.n_vocab = 11\n",
    "        self.n_dim = 64\n",
    "        self.n_head = 4\n",
    "        self.n_grid_layer = 1\n",
    "        self.n_latent_layer = 1\n",
    "        self.n_codes = 64\n",
    "        self.codebook_size = 512\n",
    "        self.height = H\n",
    "        self.width = W\n",
    "        self.n_pos = H*W\n",
    "        self.rope_base_height = 10000\n",
    "        self.rope_base_width = 10000\n",
    "\n",
    "class GridDVAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_pos = config.n_pos\n",
    "        self.embd = nn.Embedding(config.n_vocab, config.n_dim)\n",
    "        nn.init.normal_(self.embd.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "\n",
    "        ## The choice of out_norm is inspired by Llama. We can think of both Encoder and Decoder as Llama models.\n",
    "        ## With the difference that some of the layers are replaced by TransformerProjection blocks.\n",
    "        ## Like in Llama, the token embeddings flow through unnormalized until the head is applied.\n",
    "        ## In my case, we normalise the final output of the encoder as well as that of decoder before applyin their\n",
    "        ## respective heads. Nothing gets normalised from base to bottleneck and vice versa.\n",
    "\n",
    "        rope_2d = RoPE2D(\n",
    "                dim=config.n_dim // config.n_head,  # per-head dimension (e.g., 256//8 = 32)\n",
    "                max_height=config.height,\n",
    "                max_width=config.width,\n",
    "                base_height=config.rope_base_height,\n",
    "                base_width=config.rope_base_width)\n",
    "        \n",
    "        rope_1d = RotaryPositionalEmbeddings(\n",
    "            dim=config.n_dim // config.n_head,  # 256//8 = 32, per-head dimension\n",
    "            max_seq_len=config.n_pos,\n",
    "            base=config.rope_base_height\n",
    "        )\n",
    "\n",
    "        rope_codebook = RotaryPositionalEmbeddings(\n",
    "            dim=config.n_dim // 1,\n",
    "            max_seq_len=config.n_pos,\n",
    "            base=config.rope_base_height\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.grid_encoder = Transformer(\n",
    "            d_model=config.n_dim,\n",
    "            n_head=config.n_head,\n",
    "            n_layer=config.n_grid_layer,\n",
    "            out_norm=False,\n",
    "            rope=rope_2d\n",
    "        )\n",
    "\n",
    "        self.latent_encoder = LatentTransformer(\n",
    "            n_latent=config.n_codes,\n",
    "            d_model=config.n_dim,\n",
    "            n_head=config.n_head,\n",
    "            n_layer=config.n_latent_layer,\n",
    "            out_norm=False,\n",
    "            rope=rope_1d\n",
    "        )\n",
    "\n",
    "        self.codebook = AttnCodebook(d_model=config.n_dim, \n",
    "                                    codebook_size=config.codebook_size,\n",
    "                                    use_exp_relaxed=False,\n",
    "                                    rope=rope_codebook,\n",
    "                                    sampling=False,\n",
    "                                    normalise_kq=False)\n",
    "\n",
    "        self.latent_decoder = LatentTransformer(\n",
    "            n_latent=config.n_pos,\n",
    "            d_model=config.n_dim,\n",
    "            n_head=config.n_head,\n",
    "            n_layer=config.n_latent_layer,\n",
    "            out_norm=False,\n",
    "            rope=rope_1d\n",
    "        )\n",
    "\n",
    "        self.grid_decoder = Transformer(\n",
    "            d_model=config.n_dim,\n",
    "            n_head=config.n_head,\n",
    "            n_layer=config.n_grid_layer,\n",
    "            out_norm=True,\n",
    "            rope=rope_2d\n",
    "        )\n",
    "\n",
    "        self.decoder_head = nn.Linear(config.n_dim, config.n_vocab, bias=False)\n",
    "\n",
    "        rows = torch.arange(config.height, dtype=torch.long)\n",
    "        cols = torch.arange(config.width, dtype=torch.long)\n",
    "        grid_y, grid_x = torch.meshgrid(rows, cols, indexing='ij')\n",
    "        grid_pos_indices = torch.stack([grid_y.flatten(), grid_x.flatten()], dim=1).unsqueeze(0)\n",
    "        latent_pos_indices = torch.arange(config.n_pos).unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"latent_pos_indices\", latent_pos_indices, persistent=False)\n",
    "        self.register_buffer('grid_pos_indices', grid_pos_indices, persistent=False)\n",
    "\n",
    "        # Apply weight initialization on registered modules.\n",
    "        self.apply(self._init_weights)\n",
    "        # Additionally, initialize any raw nn.Parameters.\n",
    "        self.initialize_all_parameters()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # Get initialization mode from config (if present). Default is \"normal\".\n",
    "        # Set self.config.init_mode = 'xavier' if you prefer Xavier initialization.\n",
    "        init_mode = getattr(self.config, \"init_mode\", \"normal\")\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            if init_mode == \"xavier\":\n",
    "                torch.nn.init.xavier_normal_(module.weight)\n",
    "            else:\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            module.weight._initialized = True\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                module.bias._initialized = True\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            if init_mode == \"xavier\":\n",
    "                torch.nn.init.xavier_normal_(module.weight)\n",
    "            else:\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            module.weight._initialized = True\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            # Often LayerNorm weights are initialized to ones and biases to zeros.\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            module.weight._initialized = True\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                module.bias._initialized = True\n",
    "\n",
    "    def initialize_all_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize all parameters in the model recursively.\n",
    "        This method is meant to also initialize raw nn.Parameter attributes that are not part\n",
    "        of a submodule (and hence not handled by self.apply).\n",
    "        \"\"\"\n",
    "        init_mode = getattr(self.config, \"init_mode\", \"normal\")\n",
    "        for name, param in self.named_parameters():\n",
    "            # If the parameter has already been initialized (flagged via _initialized), skip.\n",
    "            if hasattr(param, \"_initialized\"):\n",
    "                continue\n",
    "            if param.ndim >= 2:\n",
    "                if init_mode == \"xavier\":\n",
    "                    torch.nn.init.xavier_normal_(param)\n",
    "                else:\n",
    "                    torch.nn.init.normal_(param, mean=0.0, std=0.02)\n",
    "            param._initialized = True\n",
    "\n",
    "    def encode(self, x: Tensor, grid_pos_indices: Tensor, latent_pos_indices: Tensor) -> Tensor:\n",
    "        x_embd = self.embd(x)\n",
    "        grid_encoded, _ = self.grid_encoder(x_embd, positions=grid_pos_indices)\n",
    "        latent_encoded, _ = self.latent_encoder(grid_encoded, positions=latent_pos_indices)\n",
    "        return latent_encoded\n",
    "\n",
    "\n",
    "    def decode(self, x: Tensor, grid_pos_indices: Tensor, latent_pos_indices: Tensor) -> Tensor:    \n",
    "        latent_decoded, _ = self.latent_decoder(x, positions=latent_pos_indices)        \n",
    "        grid_decoded, _ = self.grid_decoder(latent_decoded, positions=grid_pos_indices)        \n",
    "        grid_decoded_logits = self.decoder_head(grid_decoded)\n",
    "        return grid_decoded_logits\n",
    "\n",
    "    def forward(self, x: Tensor, tau: Tensor = torch.tensor(1.0), residual_scaling: Tensor = torch.tensor(0.0)) -> Tuple[Tensor, dict, Tensor]:\n",
    "        B, S = x.size()\n",
    "        grid_pos_indices = self.grid_pos_indices.expand(B, -1, -1)\n",
    "        latent_pos_indices = self.latent_pos_indices.expand(B, -1)\n",
    "    \n",
    "        encoded_logits = self.encode(x, grid_pos_indices, latent_pos_indices)\n",
    "    \n",
    "        quantized, log_alpha, z = self.codebook(encoded_logits, tau=tau, residual_scaling=residual_scaling, positions=latent_pos_indices)\n",
    "    \n",
    "        decoded_logits = self.decode(quantized, grid_pos_indices, latent_pos_indices)\n",
    "    \n",
    "        return decoded_logits, log_alpha\n",
    "    \n",
    "\n",
    "model = GridDVAE(GridDVAEConfig()).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(log_alpha, tau):\n",
    "    \"\"\"\n",
    "    Compute perplexity of probability distributions, accounting for temperature.\n",
    "    Perplexity = 2^(entropy), measures how uniform the distribution is.\n",
    "    \n",
    "    Args:\n",
    "        log_alpha: Logits for the latent distribution [B, N, codebook_size]\n",
    "        tau: Temperature parameter for scaling logits\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity per sample [B, N]\n",
    "    \"\"\"\n",
    "    # Apply temperature to logits and compute probabilities\n",
    "    probs = F.softmax(log_alpha / tau, dim=-1)\n",
    "    \n",
    "    # Compute entropy: -sum(p * log(p))\n",
    "    log_probs = torch.log2(probs + 1e-10)  # Add small epsilon to avoid log(0)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1)  # [B, N]\n",
    "    \n",
    "    # Perplexity = 2^(entropy)\n",
    "    perplexity = 2.0 ** entropy\n",
    "    \n",
    "    return perplexity\\\n",
    "    \n",
    "\n",
    "def entropy_loss(log_alpha, tau, reduction=\"mean\"):\n",
    "    \"\"\"\n",
    "    Compute the entropy of the latent distribution, accounting for temperature.\n",
    "    \n",
    "    Args:\n",
    "        log_alpha: Logits for the latent distribution, shape [B, N, codebook_size]\n",
    "        tau: Temperature parameter for scaling logits\n",
    "        reduction: Reduction method ('sum', 'mean', or 'batchmean')\n",
    "                \n",
    "    Returns:\n",
    "        Entropy reduced according to the specified method\n",
    "    \"\"\"\n",
    "    # Apply temperature to logits\n",
    "    scaled_log_alpha = log_alpha / tau\n",
    "    \n",
    "    # Compute log probabilities using log_softmax\n",
    "    log_probs = F.log_softmax(scaled_log_alpha, dim=-1)\n",
    "    \n",
    "    # Get probabilities by exponentiating log probabilities\n",
    "    probs = torch.exp(log_probs)\n",
    "    \n",
    "    # Compute entropy: -sum(p * log(p))\n",
    "    entropy_per_sample = -torch.sum(probs * log_probs, dim=-1)  # [B, N]\n",
    "    \n",
    "    # Apply reduction\n",
    "    if reduction == \"sum\":\n",
    "        return entropy_per_sample.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        return entropy_per_sample.mean()\n",
    "    elif reduction == \"batchmean\":\n",
    "        return entropy_per_sample.mean()\n",
    "    elif reduction == \"none\":\n",
    "        return entropy_per_sample\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid reduction: {reduction}\")\n",
    "\n",
    "def codebook_diversity_loss(log_alpha, tau, reduction=\"mean\"):\n",
    "    \"\"\"\n",
    "    Compute a diversity loss that encourages different samples to use different codebook entries.\n",
    "    \n",
    "    Args:\n",
    "        log_alpha: Logits for the latent distribution, shape [B, N, codebook_size]\n",
    "        tau: Temperature parameter for scaling logits\n",
    "        reduction: Reduction method ('sum', 'mean', or 'batchmean')\n",
    "                \n",
    "    Returns:\n",
    "        Diversity loss reduced according to the specified method\n",
    "    \"\"\"\n",
    "    # Apply temperature to logits\n",
    "    scaled_log_alpha = log_alpha / tau\n",
    "    \n",
    "    # Compute probabilities\n",
    "    probs = F.softmax(scaled_log_alpha, dim=-1)  # [B, N, codebook_size]\n",
    "    \n",
    "    # Average usage of each codebook entry across the batch\n",
    "    # This gives us the average probability of each codebook entry for each code position\n",
    "    batch_avg_probs = probs.mean(dim=0)  # [N, codebook_size]\n",
    "    \n",
    "    # Compute entropy of the batch-averaged distribution\n",
    "    # High entropy means different samples use different codebook entries\n",
    "    # Low entropy means all samples use the same codebook entries\n",
    "    log_batch_avg_probs = torch.log2(batch_avg_probs + 1e-10)\n",
    "    batch_entropy = -torch.sum(batch_avg_probs * log_batch_avg_probs, dim=-1)  # [N]\n",
    "    \n",
    "    # We want to maximize this entropy, so we negate it for minimization\n",
    "    diversity_loss = -batch_entropy  # [N]\n",
    "    \n",
    "    # Apply reduction\n",
    "    if reduction == \"sum\":\n",
    "        return diversity_loss.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        return diversity_loss.mean()\n",
    "    elif reduction == \"batchmean\":\n",
    "        return diversity_loss.mean()\n",
    "    elif reduction == \"none\":\n",
    "        return diversity_loss\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid reduction: {reduction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [00:11<15:36,  1.06it/s, loss=0.00558, recon=0.899, entropy=6.24, diversity=-9, temp=1, entropy_weight=0.0011]\n",
      " 18%|█▊        | 181/1000 [01:20<06:32,  2.09it/s, loss=-0.231, recon=0.556, entropy=6.24, diversity=-9, temp=1, entropy_weight=0.018]   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m entropy_weight \u001b[38;5;241m*\u001b[39m ent_loss \u001b[38;5;241m+\u001b[39m diversity_weight \u001b[38;5;241m*\u001b[39m div_loss\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 42\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/pips/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/pips/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.pyenv/versions/pips/lib/python3.10/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/pips/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/pips/lib/python3.10/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/pips/lib/python3.10/site-packages/torch/optim/adam.py:476\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    474\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    478\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "num_steps = 1000\n",
    "diversity_weight = 0.1\n",
    "max_entropy_weight = 0.1\n",
    "min_entropy_weight = 0.0\n",
    "entropy_anneal_steps = 1000\n",
    "\n",
    "model.train()\n",
    "\n",
    "tau = torch.tensor(1.0).to(device)\n",
    "residual_scaling = torch.tensor(0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "entropy_schedule = lambda step: min(max_entropy_weight,\n",
    "                                    min_entropy_weight + (max_entropy_weight - min_entropy_weight) * \n",
    "                                    min(1.0, step / entropy_anneal_steps))\n",
    "\n",
    "# Add a progress bar\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "step = 0\n",
    "while True:\n",
    "    for batch in train_loader:\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, log_alpha = model(batch, tau=tau, residual_scaling=residual_scaling)\n",
    "        recon_loss = criterion(logits.view(-1, logits.size(-1)), batch.view(-1))\n",
    "        ent_loss = entropy_loss(log_alpha, tau=tau, reduction=\"mean\")\n",
    "        div_loss = codebook_diversity_loss(log_alpha, tau=tau, reduction=\"mean\")\n",
    "        entropy_weight = entropy_schedule(step)\n",
    "\n",
    "        loss = recon_loss + entropy_weight * ent_loss + diversity_weight * div_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        step += 1\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'recon': recon_loss.item(),\n",
    "            'entropy': ent_loss.item(),\n",
    "            'diversity': div_loss.item() if diversity_weight > 0 else 0.0,\n",
    "            'temp': tau.item(),\n",
    "            'entropy_weight': entropy_weight\n",
    "        })\n",
    "\n",
    "        if iter >= num_steps:\n",
    "            break\n",
    "      \n",
    "\n",
    "model(batch)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
