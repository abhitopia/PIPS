{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pips.grid_dataset import GridDataset, DatasetType\n",
    "\n",
    "pad_token_id = 10\n",
    "batch_size = 32\n",
    "H, W = 32, 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GridDataset(dataset_type=DatasetType.TRAIN)\n",
    "val_dataset = GridDataset(dataset_type=DatasetType.VAL)\n",
    "\n",
    "def collate_fn(batch, permute=False):\n",
    "    result = [] \n",
    "\n",
    "    for g in batch:\n",
    "        if permute: \n",
    "            g = g.permute()\n",
    "        result.append(g.project(H, W, pad_token_id).flatten())\n",
    "\n",
    "    batch = np.stack(result)\n",
    "\n",
    "    return torch.from_numpy(batch).to(device)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          collate_fn=lambda x: collate_fn(x, permute=True))\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size, \n",
    "                        collate_fn=lambda x: collate_fn(x, permute=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pips.dvae import AttnCodebook, LatentTransformer, RoPE2D, RotaryPositionalEmbeddings, Transformer\n",
    "\n",
    "\n",
    "class GridDVAEConfig:\n",
    "    def __init__(self):\n",
    "        self.n_vocab = 11\n",
    "        self.n_dim = 64\n",
    "        self.n_head = 4\n",
    "        self.n_grid_layer = 1\n",
    "        self.n_latent_layer = 1\n",
    "        self.n_codes = 64\n",
    "        self.codebook_size = 512\n",
    "        self.height = H\n",
    "        self.width = W\n",
    "        self.n_pos = H*W\n",
    "        self.rope_base_height = 10000\n",
    "        self.rope_base_width = 10000\n",
    "\n",
    "class GridDVAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_pos = config.n_pos\n",
    "        self.embd = nn.Embedding(config.n_vocab, config.n_dim)\n",
    "        nn.init.normal_(self.embd.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "\n",
    "        ## The choice of out_norm is inspired by Llama. We can think of both Encoder and Decoder as Llama models.\n",
    "        ## With the difference that some of the layers are replaced by TransformerProjection blocks.\n",
    "        ## Like in Llama, the token embeddings flow through unnormalized until the head is applied.\n",
    "        ## In my case, we normalise the final output of the encoder as well as that of decoder before applyin their\n",
    "        ## respective heads. Nothing gets normalised from base to bottleneck and vice versa.\n",
    "\n",
    "        rope_2d = RoPE2D(\n",
    "                dim=config.n_dim // config.n_head,  # per-head dimension (e.g., 256//8 = 32)\n",
    "                max_height=config.height,\n",
    "                max_width=config.width,\n",
    "                base_height=config.rope_base_height,\n",
    "                base_width=config.rope_base_width)\n",
    "        \n",
    "        rope_1d = RotaryPositionalEmbeddings(\n",
    "            dim=config.n_dim // config.n_head,  # 256//8 = 32, per-head dimension\n",
    "            max_seq_len=config.n_pos,\n",
    "            base=config.rope_base_height\n",
    "        )\n",
    "\n",
    "        rope_codebook = RotaryPositionalEmbeddings(\n",
    "            dim=config.n_dim // 1,\n",
    "            max_seq_len=config.n_pos,\n",
    "            base=config.rope_base_height\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.grid_encoder = Transformer(\n",
    "            d_model=config.n_dim,\n",
    "            n_head=config.n_head,\n",
    "            n_layer=config.n_grid_layer,\n",
    "            out_norm=False,\n",
    "            rope=rope_2d\n",
    "        )\n",
    "\n",
    "        self.latent_encoder = LatentTransformer(\n",
    "            n_latent=config.n_codes,\n",
    "            d_model=config.n_dim,\n",
    "            n_head=config.n_head,\n",
    "            n_layer=config.n_latent_layer,\n",
    "            out_norm=False,\n",
    "            rope=rope_1d\n",
    "        )\n",
    "\n",
    "        self.codebook = AttnCodebook(d_model=config.n_dim, \n",
    "                                    codebook_size=config.codebook_size,\n",
    "                                    use_exp_relaxed=False,\n",
    "                                    rope=rope_codebook,\n",
    "                                    sampling=False,\n",
    "                                    normalise_kq=False)\n",
    "\n",
    "        self.latent_decoder = LatentTransformer(\n",
    "            n_latent=config.n_pos,\n",
    "            d_model=config.n_dim,\n",
    "            n_head=config.n_head,\n",
    "            n_layer=config.n_latent_layer,\n",
    "            out_norm=False,\n",
    "            rope=rope_1d\n",
    "        )\n",
    "\n",
    "        self.grid_decoder = Transformer(\n",
    "            d_model=config.n_dim,\n",
    "            n_head=config.n_head,\n",
    "            n_layer=config.n_grid_layer,\n",
    "            out_norm=True,\n",
    "            rope=rope_2d\n",
    "        )\n",
    "\n",
    "        self.decoder_head = nn.Linear(config.n_dim, config.n_vocab, bias=False)\n",
    "\n",
    "        rows = torch.arange(config.height, dtype=torch.long)\n",
    "        cols = torch.arange(config.width, dtype=torch.long)\n",
    "        grid_y, grid_x = torch.meshgrid(rows, cols, indexing='ij')\n",
    "        grid_pos_indices = torch.stack([grid_y.flatten(), grid_x.flatten()], dim=1).unsqueeze(0)\n",
    "        latent_pos_indices = torch.arange(config.n_pos).unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"latent_pos_indices\", latent_pos_indices, persistent=False)\n",
    "        self.register_buffer('grid_pos_indices', grid_pos_indices, persistent=False)\n",
    "\n",
    "        # Apply weight initialization on registered modules.\n",
    "        # self.apply(self._init_weights)\n",
    "        # Additionally, initialize any raw nn.Parameters.\n",
    "        # self.initialize_all_parameters()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # Get initialization mode from config (if present). Default is \"normal\".\n",
    "        # Set self.config.init_mode = 'xavier' if you prefer Xavier initialization.\n",
    "        init_mode = getattr(self.config, \"init_mode\", \"normal\")\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            if init_mode == \"xavier\":\n",
    "                torch.nn.init.xavier_normal_(module.weight)\n",
    "            else:\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            module.weight._initialized = True\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                module.bias._initialized = True\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            if init_mode == \"xavier\":\n",
    "                torch.nn.init.xavier_normal_(module.weight)\n",
    "            else:\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            module.weight._initialized = True\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            # Often LayerNorm weights are initialized to ones and biases to zeros.\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            module.weight._initialized = True\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                module.bias._initialized = True\n",
    "\n",
    "    def initialize_all_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize all parameters in the model recursively.\n",
    "        This method is meant to also initialize raw nn.Parameter attributes that are not part\n",
    "        of a submodule (and hence not handled by self.apply).\n",
    "        \"\"\"\n",
    "        init_mode = getattr(self.config, \"init_mode\", \"normal\")\n",
    "        for name, param in self.named_parameters():\n",
    "            # If the parameter has already been initialized (flagged via _initialized), skip.\n",
    "            if hasattr(param, \"_initialized\"):\n",
    "                continue\n",
    "            if param.ndim >= 2:\n",
    "                if init_mode == \"xavier\":\n",
    "                    torch.nn.init.xavier_normal_(param)\n",
    "                else:\n",
    "                    torch.nn.init.normal_(param, mean=0.0, std=0.02)\n",
    "            param._initialized = True\n",
    "\n",
    "    def encode(self, x: Tensor, grid_pos_indices: Tensor, latent_pos_indices: Tensor) -> Tensor:\n",
    "        x_embd = self.embd(x)\n",
    "        grid_encoded, _ = self.grid_encoder(x_embd, positions=grid_pos_indices)\n",
    "        latent_encoded, _ = self.latent_encoder(grid_encoded, positions=latent_pos_indices)\n",
    "        return latent_encoded\n",
    "\n",
    "\n",
    "    def decode(self, x: Tensor, grid_pos_indices: Tensor, latent_pos_indices: Tensor) -> Tensor:    \n",
    "        latent_decoded, _ = self.latent_decoder(x, positions=latent_pos_indices)        \n",
    "        grid_decoded, _ = self.grid_decoder(latent_decoded, positions=grid_pos_indices)        \n",
    "        grid_decoded_logits = self.decoder_head(grid_decoded)\n",
    "        return grid_decoded_logits\n",
    "\n",
    "    def forward(self, x: Tensor, tau: Tensor = torch.tensor(1.0), residual_scaling: Tensor = torch.tensor(0.0)) -> Tuple[Tensor, dict, Tensor]:\n",
    "        B, S = x.size()\n",
    "        grid_pos_indices = self.grid_pos_indices.expand(B, -1, -1)\n",
    "        latent_pos_indices = self.latent_pos_indices.expand(B, -1)\n",
    "    \n",
    "        encoded_logits = self.encode(x, grid_pos_indices, latent_pos_indices)\n",
    "    \n",
    "        quantized, log_alpha, z = self.codebook(encoded_logits, tau=tau, residual_scaling=residual_scaling, positions=latent_pos_indices)\n",
    "    \n",
    "        decoded_logits = self.decode(quantized, grid_pos_indices, latent_pos_indices)\n",
    "    \n",
    "        return decoded_logits, log_alpha\n",
    "    \n",
    "\n",
    "model = GridDVAE(GridDVAEConfig()).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(log_alpha, tau):\n",
    "    \"\"\"\n",
    "    Compute perplexity of probability distributions, accounting for temperature.\n",
    "    Perplexity = 2^(entropy), measures how uniform the distribution is.\n",
    "    \n",
    "    Args:\n",
    "        log_alpha: Logits for the latent distribution [B, N, codebook_size]\n",
    "        tau: Temperature parameter for scaling logits\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity per sample [B, N]\n",
    "    \"\"\"\n",
    "    # Apply temperature to logits and compute probabilities\n",
    "    probs = F.softmax(log_alpha / tau, dim=-1)\n",
    "    \n",
    "    # Compute entropy: -sum(p * log(p))\n",
    "    log_probs = torch.log2(probs + 1e-10)  # Add small epsilon to avoid log(0)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1)  # [B, N]\n",
    "    \n",
    "    # Perplexity = 2^(entropy)\n",
    "    perplexity = 2.0 ** entropy\n",
    "    \n",
    "    return perplexity\\\n",
    "    \n",
    "\n",
    "def entropy_loss(log_alpha, tau, reduction=\"mean\"):\n",
    "    \"\"\"\n",
    "    Compute the entropy of the latent distribution, accounting for temperature.\n",
    "    \n",
    "    Args:\n",
    "        log_alpha: Logits for the latent distribution, shape [B, N, codebook_size]\n",
    "        tau: Temperature parameter for scaling logits\n",
    "        reduction: Reduction method ('sum', 'mean', or 'batchmean')\n",
    "                \n",
    "    Returns:\n",
    "        Entropy reduced according to the specified method\n",
    "    \"\"\"\n",
    "    # Apply temperature to logits\n",
    "    scaled_log_alpha = log_alpha / tau\n",
    "    \n",
    "    # Compute log probabilities using log_softmax\n",
    "    log_probs = F.log_softmax(scaled_log_alpha, dim=-1)\n",
    "    \n",
    "    # Get probabilities by exponentiating log probabilities\n",
    "    probs = torch.exp(log_probs)\n",
    "    \n",
    "    # Compute entropy: -sum(p * log(p))\n",
    "    entropy_per_sample = -torch.sum(probs * log_probs, dim=-1)  # [B, N]\n",
    "    \n",
    "    # Apply reduction\n",
    "    if reduction == \"sum\":\n",
    "        return entropy_per_sample.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        return entropy_per_sample.mean()\n",
    "    elif reduction == \"batchmean\":\n",
    "        return entropy_per_sample.mean()\n",
    "    elif reduction == \"none\":\n",
    "        return entropy_per_sample\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid reduction: {reduction}\")\n",
    "\n",
    "def codebook_diversity_loss(log_alpha, tau, reduction=\"mean\"):\n",
    "    \"\"\"\n",
    "    Compute a diversity loss that encourages different samples to use different codebook entries.\n",
    "    \n",
    "    Args:\n",
    "        log_alpha: Logits for the latent distribution, shape [B, N, codebook_size]\n",
    "        tau: Temperature parameter for scaling logits\n",
    "        reduction: Reduction method ('sum', 'mean', or 'batchmean')\n",
    "                \n",
    "    Returns:\n",
    "        Diversity loss reduced according to the specified method\n",
    "    \"\"\"\n",
    "    # Apply temperature to logits\n",
    "    scaled_log_alpha = log_alpha / tau\n",
    "    \n",
    "    # Compute probabilities\n",
    "    probs = F.softmax(scaled_log_alpha, dim=-1)  # [B, N, codebook_size]\n",
    "    \n",
    "    # Average usage of each codebook entry across the batch\n",
    "    # This gives us the average probability of each codebook entry for each code position\n",
    "    batch_avg_probs = probs.mean(dim=0)  # [N, codebook_size]\n",
    "    \n",
    "    # Compute entropy of the batch-averaged distribution\n",
    "    # High entropy means different samples use different codebook entries\n",
    "    # Low entropy means all samples use the same codebook entries\n",
    "    log_batch_avg_probs = torch.log2(batch_avg_probs + 1e-10)\n",
    "    batch_entropy = -torch.sum(batch_avg_probs * log_batch_avg_probs, dim=-1)  # [N]\n",
    "    \n",
    "    # We want to maximize this entropy, so we negate it for minimization\n",
    "    diversity_loss = -batch_entropy  # [N]\n",
    "    \n",
    "    # Apply reduction\n",
    "    if reduction == \"sum\":\n",
    "        return diversity_loss.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        return diversity_loss.mean()\n",
    "    elif reduction == \"batchmean\":\n",
    "        return diversity_loss.mean()\n",
    "    elif reduction == \"none\":\n",
    "        return diversity_loss\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid reduction: {reduction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:09<21:31,  1.30s/it, loss=-0.444, recon=0.444, entropy=6.18, diversity=-8.92, temp=1, entropy_weight=0.0006]\n",
      "24778it [4:52:03,  1.84it/s, loss=-0.15, recon=0.124, entropy=0.00897, diversity=-2.75, temp=1, entropy_weight=0.1]                       "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "num_steps = 1000\n",
    "diversity_weight = 0.1\n",
    "max_entropy_weight = 0.1\n",
    "min_entropy_weight = 0.0\n",
    "entropy_anneal_steps = 1000\n",
    "\n",
    "model.train()\n",
    "\n",
    "tau = torch.tensor(1.0).to(device)\n",
    "residual_scaling = torch.tensor(0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "entropy_schedule = lambda step: min(max_entropy_weight,\n",
    "                                    min_entropy_weight + (max_entropy_weight - min_entropy_weight) * \n",
    "                                    min(1.0, step / entropy_anneal_steps))\n",
    "\n",
    "# Add a progress bar\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "step = 0\n",
    "while True:\n",
    "    for batch in train_loader:\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, log_alpha = model(batch, tau=tau, residual_scaling=residual_scaling)\n",
    "        recon_loss = criterion(logits.view(-1, logits.size(-1)), batch.view(-1))\n",
    "        ent_loss = entropy_loss(log_alpha, tau=tau, reduction=\"mean\")\n",
    "        div_loss = codebook_diversity_loss(log_alpha, tau=tau, reduction=\"mean\")\n",
    "        entropy_weight = entropy_schedule(step)\n",
    "\n",
    "        loss = recon_loss + entropy_weight * ent_loss + diversity_weight * div_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        step += 1\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'recon': recon_loss.item(),\n",
    "            'entropy': ent_loss.item(),\n",
    "            'diversity': div_loss.item() if diversity_weight > 0 else 0.0,\n",
    "            'temp': tau.item(),\n",
    "            'entropy_weight': entropy_weight\n",
    "        })\n",
    "\n",
    "        if iter >= num_steps:\n",
    "            break\n",
    "      \n",
    "\n",
    "model(batch)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
