

## Training
- [x] Implement KLD Loss
- [ ] Implement beta-VAE
- [ ] Implement code usage monitor
- [ ] Implement Tau Scheduling
- [ ] Implement Gumbel Hard Scheduling
- [ ] Implement Throughput Monitor
- [ ] Save hyperparameters
- [ ] Training Resume
- [ ] Control WandB initialisation and naming
- [ ] Implement Masking Schedule
- [ ] Exponential Weighted Iterate Average for parameters

## Disentanglement
- [ ] Implement explicit disentanglement in KLD Loss
- [ ] Implement disentanglement monitor

## CLI
- [ ] Add Seeding Everything
- [ ] Check the Dalle-E Paper for default parameters and schedules
- [ ] Check Dalle-E paper for how they handle loss component scaling

## Refactor
- [ ] Remove pesky warnings

## Optimisation
- [ ] Optimise training for speed

## Grokking
- [ ] Implement Orthograd Optimizer
- [ ] Use 32bit float CE loss