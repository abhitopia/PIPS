experiment_config:
  model_config:
    n_dim: 256
    n_head: 4
    n_latent: 64
    activation: gelu
    n_vocab: 16
    n_layer_encoder_grid: 2
    n_layer_encoder_latent: 2
    n_layer_program: 6
    n_iter: 1
    grid_height: 32
    grid_width: 32
    dropout: 0.0
    rope_base: 10000
  seed: 42
  batch_size: 512
  max_steps: 100000
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  learning_rate: 0.001
  lr_min: 1.0e-05
  program_embedding_lr_multiplier: 0.1
  warmup_steps_lr: 1000
  decay_steps_lr: 99000
  adamw_betas_1: 0.9
  adamw_betas_2: 0.999
  weight_decay: 0.0001
  noise_eta: 0.3
  noise_gamma: 0.55
  beta_reconstruction: 0.0
  beta_prediction: 1.0
  dataset: ALL_SMALL
  group_by_program: true
  limit_training_samples: null
  limit_validation_samples: 50000
  data_multiplier: 8
  mask_pct: 0.0
  mask_transition_steps: 10000
  model_src: v2/backup/51060
  train_only_program_embeddings: false
  freeze_autoencoder: true
  em_start_epoch: null
project_config:
  run_name: v8_noise_v1
  project_name: prosip-auto
  checkpoint_dir: prosip_runs
  val_check_interval: 1000
